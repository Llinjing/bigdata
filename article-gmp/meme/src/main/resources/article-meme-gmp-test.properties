config.type = meme-config-test

## kafka
src.kafka.zk.list = 10.10.20.14:2181,10.10.20.15:2181,10.10.20.16:2181
src.kafka.broker.list = 10.10.20.14:9092,10.10.20.15:9092,10.10.20.16:9092
src.kafka.consumer.group = test
src.kafka.batch.size = 2000
src.kafka.sync = false

## topic

topic.meme = thumbToDashboard
topic.impression = impression-reformat
topic.article.meme.gmp = test
topic.meme.partition = 9
topic.impression.partition = 9
topic.article.meme.gmp.partition = 9

## spark streaming
sparkstreaming.batch.size.minutes = 10
sparkstreaming.window.num = 1
sparkstreaming.checkpoint = s3://bigdata-east/user/haifeng.huang/spark/checkpoint
sparkstreaming.zk.offset.path = /test/huanghaifeng/offset
sparkstreaming.zk.finish.mark.path = /test/huanghaifeng/finish/mark

## hdfs
hdfs.meme = s3://bigdata-east/user/haifeng.huang/spark/data/request
hdfs.impression = s3://bigdata-east/user/haifeng.huang/spark/data/impression
hdfs.article.meme.gmp = s3://bigdata-east/user/haifeng.huang/spark/data/article-gmp
hdfs.total.decay.feedback = s3://bigdata-east/user/haifeng.huang/spark/data/total-decay-feedback
hdfs.redis = s3://bigdata-east/user/haifeng.huang/spark/data/redis

## decay
decay_ratio = 0.995
impression_threshold = 100

valid_hour = 48
date_format = yyyyMMddHH

## key tag
key_tag = ###

## redis
redis.servers = 10.10.100.100:6300,10.10.100.101:6300,10.10.100.102:6300,10.10.100.103:6301,10.10.100.104:6301,10.10.100.105:6301
redis.cluster = true
redis.hash.key.prefix = article-meme-gmp-spark-streaming-hash-key
redis.hash.key.partitions = 100

## debug
debug = true
hdfs_debug = s3://bigdata-east/user/haifeng.huang/spark/data/debug
