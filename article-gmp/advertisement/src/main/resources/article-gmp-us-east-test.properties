config.type = us-east-test-config

## kafka
src.kafka.zk.list = 10.10.20.14:2181,10.10.20.15:2181,10.10.20.16:2181
src.kafka.broker.list = 10.10.20.14:9092,10.10.20.15:9092,10.10.20.16:9092
src.kafka.consumer.group = test
src.kafka.batch.size = 2000
src.kafka.sync = false

## topic
topic.click = click-reformat
topic.impression = impression-reformat
topic.article.gmp = test
topic.click.partition = 9
topic.impression.partition = 9
topic.article.gmp.partition = 6

## spark streaming
sparkstreaming.batch.size.minutes = 10
sparkstreaming.window.num = 1
sparkstreaming.checkpoint = s3://bigdata-east/user/haifeng.huang/spark/checkpoint
sparkstreaming.zk.offset.path = /test/huanghaifeng/offset
sparkstreaming.zk.finish.mark.path = /test/huanghaifeng/finish/mark

## hdfs
hdfs.impression = s3://bigdata-east/user/haifeng.huang/spark/data/impression
hdfs.article.gmp = s3://bigdata-east/user/haifeng.huang/spark/data/advertisement-gmp
hdfs.total.decay.feedback = s3://bigdata-east/user/haifeng.huang/spark/data/total-decay-feedback
hdfs.redis = s3://bigdata-east/user/haifeng.huang/spark/data/redis

## decay
decay_ratio = 0.995
impression_threshold = 100

valid_hour = 48
date_format = yyyyMMddHH

## key tag
key_tag = ###
    
## redis
redis.cluster = true
redis.servers = 10.10.100.100:6300,10.10.100.101:6300,10.10.100.102:6300,10.10.100.103:6301,10.10.100.104:6301,10.10.100.105:6301
redis.port = 6300
redis.password =
redis.key.prefix = test-ad-gmp-
redis.key.ttl = 172800
redis.key.partitions = 1
redis.increment.write = true

## debug
debug = true
hdfs_debug = s3://bigdata-east/user/haifeng.huang/spark/data/debug
